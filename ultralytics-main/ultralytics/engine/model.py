# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import inspect
from pathlib import Path
from typing import Any, Dict, List, Union

import numpy as np
import torch
from PIL import Image

from ultralytics.cfg import TASK2DATA, get_cfg, get_save_dir
from ultralytics.engine.results import Results
from ultralytics.hub import HUB_WEB_ROOT, HUBTrainingSession
from ultralytics.nn.tasks import attempt_load_one_weight, guess_model_task, yaml_model_load
from ultralytics.utils import (
    ARGV,
    ASSETS,
    DEFAULT_CFG_DICT,
    LOGGER,
    RANK,
    SETTINGS,
    callbacks,
    checks,
    emojis,
    yaml_load,
)


class Model(torch.nn.Module):
    """
    ä¸€ä¸ªç”¨äºå®ç° YOLO æ¨¡å‹çš„åŸºç±»ï¼Œç»Ÿä¸€äº†ä¸åŒæ¨¡å‹ç±»å‹çš„ APIã€‚

    è¯¥ç±»ä¸ºä¸ YOLO æ¨¡å‹ç›¸å…³çš„å„ç§æ“ä½œæä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œä¾‹å¦‚è®­ç»ƒã€éªŒè¯ã€é¢„æµ‹ã€å¯¼å‡ºå’ŒåŸºå‡†æµ‹è¯•ã€‚å®ƒå¤„ç†ä¸åŒç±»å‹çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»æœ¬åœ°æ–‡ä»¶ã€Ultralytics HUB æˆ– Triton Server åŠ è½½çš„æ¨¡å‹ã€‚

    å±æ€§:
        callbacks (Dict): ä¸€ä¸ªåŒ…å«æ¨¡å‹æ“ä½œæœŸé—´å„ç§äº‹ä»¶å›è°ƒå‡½æ•°çš„å­—å…¸ã€‚
        predictor (BasePredictor): ç”¨äºè¿›è¡Œé¢„æµ‹çš„é¢„æµ‹å™¨å¯¹è±¡ã€‚
        model (torch.nn.Module): åº•å±‚çš„ PyTorch æ¨¡å‹ã€‚
        trainer (BaseTrainer): ç”¨äºè®­ç»ƒæ¨¡å‹çš„è®­ç»ƒå™¨å¯¹è±¡ã€‚
        ckpt (Dict): å¦‚æœæ¨¡å‹æ˜¯ä» *.pt æ–‡ä»¶åŠ è½½çš„ï¼Œåˆ™åŒ…å«æ£€æŸ¥ç‚¹æ•°æ®ã€‚
        cfg (str): å¦‚æœæ¨¡å‹æ˜¯ä» *.yaml æ–‡ä»¶åŠ è½½çš„ï¼Œåˆ™åŒ…å«æ¨¡å‹é…ç½®ã€‚
        ckpt_path (str): æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„ã€‚
        overrides (Dict): æ¨¡å‹é…ç½®çš„è¦†ç›–å‚æ•°å­—å…¸ã€‚
        metrics (Dict): æœ€æ–°çš„è®­ç»ƒ/éªŒè¯æŒ‡æ ‡ã€‚
        session (HUBTrainingSession): Ultralytics HUB ä¼šè¯ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚
        task (str): æ¨¡å‹æ‰€é’ˆå¯¹çš„ä»»åŠ¡ç±»å‹ã€‚
        model_name (str): æ¨¡å‹çš„åç§°ã€‚

    æ–¹æ³•:
        __call__: predict æ–¹æ³•çš„åˆ«åï¼Œä½¿æ¨¡å‹å®ä¾‹å¯ä»¥ç›´æ¥è°ƒç”¨ã€‚
        _new: æ ¹æ®é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ã€‚
        _load: ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡å‹ã€‚
        _check_is_pytorch_model: ç¡®ä¿æ¨¡å‹æ˜¯ PyTorch æ¨¡å‹ã€‚
        reset_weights: å°†æ¨¡å‹çš„æƒé‡é‡ç½®ä¸ºåˆå§‹çŠ¶æ€ã€‚
        load: ä»æŒ‡å®šæ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡ã€‚
        save: å°†æ¨¡å‹çš„å½“å‰çŠ¶æ€ä¿å­˜åˆ°æ–‡ä»¶ã€‚
        info: è®°å½•æˆ–è¿”å›æ¨¡å‹ä¿¡æ¯ã€‚
        fuse: èåˆ Conv2d å’Œ BatchNorm2d å±‚ä»¥ä¼˜åŒ–æ¨ç†ã€‚
        predict: æ‰§è¡Œç›®æ ‡æ£€æµ‹é¢„æµ‹ã€‚
        track: æ‰§è¡Œç›®æ ‡è·Ÿè¸ªã€‚
        val: åœ¨æ•°æ®é›†ä¸ŠéªŒè¯æ¨¡å‹ã€‚
        benchmark: åœ¨å„ç§å¯¼å‡ºæ ¼å¼ä¸Šå¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚
        export: å°†æ¨¡å‹å¯¼å‡ºä¸ºä¸åŒæ ¼å¼ã€‚
        train: åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ã€‚
        tune: æ‰§è¡Œè¶…å‚æ•°è°ƒä¼˜ã€‚
        _apply: å¯¹æ¨¡å‹çš„å¼ é‡åº”ç”¨å‡½æ•°ã€‚
        add_callback: ä¸ºäº‹ä»¶æ·»åŠ å›è°ƒå‡½æ•°ã€‚
        clear_callback: æ¸…é™¤äº‹ä»¶çš„æ‰€æœ‰å›è°ƒå‡½æ•°ã€‚
        reset_callbacks: å°†æ‰€æœ‰å›è°ƒå‡½æ•°é‡ç½®ä¸ºé»˜è®¤å‡½æ•°ã€‚
    """

    def __init__(
        self,
        model: Union[str, Path] = "yolo11n.pt",
        task: str = None,
        verbose: bool = True,
    ) -> None:

        print("\nModelç±»__init__()è¢«è°ƒç”¨","åœ°å€ï¼šultralytics-main\\ultralytics\\engine\\model.py")

        """
        åˆå§‹åŒ– YOLO æ¨¡å‹ç±»çš„æ–°å®ä¾‹ã€‚

        æ­¤æ„é€ å‡½æ•°æ ¹æ®æä¾›çš„æ¨¡å‹è·¯å¾„æˆ–åç§°è®¾ç½®æ¨¡å‹ã€‚å®ƒå¤„ç†å„ç§ç±»å‹çš„æ¨¡å‹æ¥æºï¼ŒåŒ…æ‹¬æœ¬åœ°æ–‡ä»¶ã€Ultralytics HUB æ¨¡å‹å’Œ Triton Server æ¨¡å‹ã€‚è¯¥æ–¹æ³•åˆå§‹åŒ–æ¨¡å‹çš„å‡ ä¸ªé‡è¦å±æ€§ï¼Œå¹¶ä¸ºå…¶å‡†å¤‡è®­ç»ƒã€é¢„æµ‹æˆ–å¯¼å‡ºç­‰æ“ä½œã€‚

        å‚æ•°:
            model (Union[str, Path]): è¦åŠ è½½æˆ–åˆ›å»ºçš„æ¨¡å‹çš„è·¯å¾„æˆ–åç§°ã€‚å¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€Ultralytics HUB çš„æ¨¡å‹åç§°æˆ– Triton Server æ¨¡å‹ã€‚
            task (str | None): ä¸ YOLO æ¨¡å‹å…³è”çš„ä»»åŠ¡ç±»å‹ï¼ŒæŒ‡å®šå…¶åº”ç”¨é¢†åŸŸã€‚
            verbose (bool): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ¨¡å‹åˆå§‹åŒ–åŠå…¶åç»­æ“ä½œæœŸé—´å¯ç”¨è¯¦ç»†è¾“å‡ºã€‚

        å¼‚å¸¸:
            FileNotFoundError: å¦‚æœæŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚
            ValueError: å¦‚æœæ¨¡å‹æ–‡ä»¶æˆ–é…ç½®æ— æ•ˆæˆ–ä¸æ”¯æŒã€‚
            ImportError: å¦‚æœç‰¹å®šæ¨¡å‹ç±»å‹ï¼ˆå¦‚ HUB SDKï¼‰æ‰€éœ€çš„ä¾èµ–é¡¹æœªå®‰è£…ã€‚

        ç¤ºä¾‹:
            >>> model = Model("yolo11n.pt")
            >>> model = Model("path/to/model.yaml", task="detect")
            >>> model = Model("hub_model", verbose=True)
        """
        super().__init__()
        self.callbacks = callbacks.get_default_callbacks()  # è·å–é»˜è®¤å›è°ƒå‡½æ•°
        self.predictor = None  # é‡ç”¨é¢„æµ‹å™¨
        self.model = None  # æ¨¡å‹å¯¹è±¡
        self.trainer = None  # è®­ç»ƒå™¨å¯¹è±¡
        self.ckpt = {}  # å¦‚æœä» *.pt æ–‡ä»¶åŠ è½½
        self.cfg = None  # å¦‚æœä» *.yaml æ–‡ä»¶åŠ è½½
        self.ckpt_path = None  # æ£€æŸ¥ç‚¹è·¯å¾„
        self.overrides = {}  # è®­ç»ƒå™¨é…ç½®çš„è¦†ç›–å‚æ•°
        self.metrics = None  # éªŒè¯/è®­ç»ƒæŒ‡æ ‡
        self.session = None  # HUB ä¼šè¯
        self.task = task  # ä»»åŠ¡ç±»å‹
        self.model_name = None  # æ¨¡å‹åç§°
        model = str(model).strip()  # å»é™¤å‰åç©ºæ ¼

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¥è‡ª https://hub.ultralytics.com çš„ Ultralytics HUB æ¨¡å‹
        if self.is_hub_model(model):
            # ä» HUB è·å–æ¨¡å‹
            checks.check_requirements("hub-sdk>=0.0.12")
            session = HUBTrainingSession.create_session(model)
            model = session.model_file
            if session.train_args:  # å¦‚æœæ˜¯ä» HUB å‘é€çš„è®­ç»ƒä»»åŠ¡
                self.session = session

        # æ£€æŸ¥æ˜¯å¦æ˜¯ Triton Server æ¨¡å‹
        elif self.is_triton_model(model):
            self.model_name = self.model = model
            self.overrides["task"] = task or "detect"  # å¦‚æœæœªæ˜ç¡®è®¾ç½®ï¼Œåˆ™é»˜è®¤ä»»åŠ¡ä¸ºæ£€æµ‹
            return

        # åŠ è½½æˆ–åˆ›å»ºæ–°çš„ YOLO æ¨¡å‹
        __import__("os").environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"  # é¿å…ç¡®å®šæ€§è­¦å‘Š
        if Path(model).suffix in {".yaml", ".yml"}:  # å¦‚æœæ˜¯ YAML é…ç½®æ–‡ä»¶
            self._new(model, task=task, verbose=verbose)
        else:  # å¦åˆ™åŠ è½½æ¨¡å‹
            self._load(model, task=task)

        # åˆ é™¤ super().training ä»¥ä¾¿è®¿é—® self.model.training
        del self.training




############################################################################################################################################################################################
############################################################################################################################################################################################
############################################################################################################################################################################################





    @staticmethod
    def is_triton_model(model: str) -> bool:
        print('Model.is_triton_model()è¢«è°ƒç”¨', 'æ£€æŸ¥æä¾›çš„æ¨¡å‹æ˜¯å¦æ˜¯ Triton Server æ¨¡å‹')
        """
        Checks if the given model string is a Triton Server URL.

        This static method determines whether the provided model string represents a valid Triton Server URL by
        parsing its components using urllib.parse.urlsplit().

        Args:
            model (str): The model string to be checked.

        Returns:
            (bool): True if the model string is a valid Triton Server URL, False otherwise.

        Examples:
            >>> Model.is_triton_model("http://localhost:8000/v2/models/yolo11n")
            True
            >>> Model.is_triton_model("yolo11n.pt")
            False
        """
        from urllib.parse import urlsplit

        url = urlsplit(model)
        return url.netloc and url.path and url.scheme in {"http", "grpc"}
############################################################################################################################################################################################

    @staticmethod
    def is_hub_model(model: str) -> bool:
        print("\nModel.is_hub_model()è¢«è°ƒç”¨",'æ£€æŸ¥æä¾›çš„æ¨¡å‹æ˜¯å¦æ˜¯ Ultralytics HUB æ¨¡å‹')

        """
        æ£€æŸ¥æä¾›çš„æ¨¡å‹æ˜¯å¦æ˜¯ Ultralytics HUB æ¨¡å‹ã€‚

        æ­¤é™æ€æ–¹æ³•ç”¨äºåˆ¤æ–­ç»™å®šçš„æ¨¡å‹å­—ç¬¦ä¸²æ˜¯å¦ä»£è¡¨ä¸€ä¸ªæœ‰æ•ˆçš„ Ultralytics HUB æ¨¡å‹æ ‡è¯†ç¬¦ã€‚

        å‚æ•°:
            model (str): è¦æ£€æŸ¥çš„æ¨¡å‹å­—ç¬¦ä¸²ã€‚

        è¿”å›:
            (bool): å¦‚æœæ¨¡å‹æ˜¯æœ‰æ•ˆçš„ Ultralytics HUB æ¨¡å‹ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚

        ç¤ºä¾‹:
            >>> Model.is_hub_model("https://hub.ultralytics.com/models/MODEL")
            True
            >>> Model.is_hub_model("yolo11n.pt")
            False
        """
        return model.startswith(f"{HUB_WEB_ROOT}/models/")
############################################################################################################################################################################################


    def _load(self, weights: str, task=None) -> None:

        print("\nModel._load()è¢«è°ƒç”¨",'åŠ è½½çš„æƒé‡è®¾ç½®æ¨¡å‹ã€ä»»åŠ¡å’Œç›¸å…³å±æ€§')

        """
        ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡å‹æˆ–ä»æƒé‡æ–‡ä»¶åˆå§‹åŒ–æ¨¡å‹ã€‚

        è¯¥æ–¹æ³•å¤„ç†ä» .pt æ£€æŸ¥ç‚¹æ–‡ä»¶æˆ–å…¶ä»–æƒé‡æ–‡ä»¶æ ¼å¼åŠ è½½æ¨¡å‹ã€‚å®ƒæ ¹æ®åŠ è½½çš„æƒé‡è®¾ç½®æ¨¡å‹ã€ä»»åŠ¡å’Œç›¸å…³å±æ€§ã€‚

        å‚æ•°:
            weights (str): è¦åŠ è½½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ã€‚
            task (str | None): ä¸æ¨¡å‹å…³è”çš„ä»»åŠ¡ç±»å‹ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä»æ¨¡å‹ä¸­æ¨æ–­ã€‚

        å¼‚å¸¸:
            FileNotFoundError: å¦‚æœæŒ‡å®šçš„æƒé‡æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚
            ValueError: å¦‚æœæƒé‡æ–‡ä»¶æ ¼å¼ä¸å—æ”¯æŒæˆ–æ— æ•ˆã€‚

        ç¤ºä¾‹:
            >>> model = Model()
            >>> model._load("yolo11n.pt")
            >>> model._load("path/to/weights.pth", task="detect")
        """
        if weights.lower().startswith(("https://", "http://", "rtsp://", "rtmp://", "tcp://")):
            weights = checks.check_file(weights, download_dir=SETTINGS["weights_dir"])  # download and return local file
        weights = checks.check_model_file_from_stem(weights)  # add suffix, i.e. yolo11n -> yolo11n.pt

        if Path(weights).suffix == ".pt":
            self.model, self.ckpt = attempt_load_one_weight(weights)
            self.task = self.model.args["task"]
            self.overrides = self.model.args = self._reset_ckpt_args(self.model.args)
            self.ckpt_path = self.model.pt_path
        else:
            weights = checks.check_file(weights)  # runs in all cases, not redundant with above call
            self.model, self.ckpt = weights, None
            self.task = task or guess_model_task(weights)
            self.ckpt_path = weights
        self.overrides["model"] = weights
        self.overrides["task"] = self.task
        self.model_name = weights
############################################################################################################################################################################################



    def predict(
        self,
        source: Union[str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        predictor=None,     # åœ¨é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œé»˜è®¤åŠ è½½ä¸º<ultralytics.models.yolo.detect.predict.DetectionPredictor object>å¯¹è±¡
        **kwargs: Any,
    ) -> List[Results]:

        print('\nModel.predict()è¢«è°ƒç”¨','è¿›è¡Œé¢„æµ‹,å³å°†è¿”å›ç»“æœ(List[ultralytics.engine.results.Results])')

        """
        ä½¿ç”¨ YOLO æ¨¡å‹å¯¹ç»™å®šçš„å›¾åƒæºè¿›è¡Œé¢„æµ‹ã€‚

        è¯¥æ–¹æ³•é€šè¿‡å…³é”®å­—å‚æ•°æ”¯æŒå„ç§é…ç½®ï¼Œç®€åŒ–äº†é¢„æµ‹è¿‡ç¨‹ã€‚å®ƒæ”¯æŒä½¿ç”¨è‡ªå®šä¹‰é¢„æµ‹å™¨æˆ–é»˜è®¤é¢„æµ‹å™¨æ–¹æ³•è¿›è¡Œé¢„æµ‹ã€‚è¯¥æ–¹æ³•å¤„ç†ä¸åŒç±»å‹çš„å›¾åƒæºï¼Œå¹¶å¯ä»¥åœ¨æµæ¨¡å¼ä¸‹è¿è¡Œã€‚

        å‚æ•°:
            source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): 
                è¦è¿›è¡Œé¢„æµ‹çš„å›¾åƒæ¥æºã€‚æ¥å—å„ç§ç±»å‹ï¼ŒåŒ…æ‹¬æ–‡ä»¶è·¯å¾„ã€URLã€PIL å›¾åƒã€numpy æ•°ç»„å’Œ torch å¼ é‡ã€‚
            stream (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†è¾“å…¥æºè§†ä¸ºè¿ç»­æµè¿›è¡Œé¢„æµ‹ã€‚
            predictor (BasePredictor | None): ç”¨äºè¿›è¡Œé¢„æµ‹çš„è‡ªå®šä¹‰é¢„æµ‹å™¨ç±»çš„å®ä¾‹ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤é¢„æµ‹å™¨ã€‚
            **kwargs: ç”¨äºé…ç½®é¢„æµ‹è¿‡ç¨‹çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            (List[ultralytics.engine.results.Results]): é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœå°è£…åœ¨ä¸€ä¸ª Results å¯¹è±¡ä¸­ã€‚

        ç¤ºä¾‹:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.predict(source="path/to/image.jpg", conf=0.25)
            >>> for r in results:
            ...     print(r.boxes.data)  # æ‰“å°æ£€æµ‹æ¡†æ•°æ®

        æ³¨æ„:
            - å¦‚æœæœªæä¾› 'source'ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ ASSETS å¸¸é‡å¹¶å‘å‡ºè­¦å‘Šã€‚
            - å¦‚æœé¢„æµ‹å™¨å°šæœªè®¾ç½®ï¼Œåˆ™è¯¥æ–¹æ³•ä¼šè®¾ç½®ä¸€ä¸ªæ–°çš„é¢„æµ‹å™¨ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æ›´æ–°å…¶å‚æ•°ã€‚
            - å¯¹äº SAM ç±»å‹çš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡å…³é”®å­—å‚æ•°ä¼ é€’ 'prompts'ã€‚
        """
        if source is None:
            source = ASSETS
            LOGGER.warning(f"WARNING âš ï¸ 'source' is missing. Using 'source={source}'.")

        is_cli = (ARGV[0].endswith("yolo") or ARGV[0].endswith("ultralytics")) and any(
            x in ARGV for x in ("predict", "track", "mode=predict", "mode=track")
        )#åˆ¤æ–­æ˜¯å¦æ˜¯å‘½ä»¤è¡Œè°ƒç”¨çš„yoloæˆ–è€…ultralyticsï¼Œå¹¶ä¸”æ˜¯å¦åŒ…å«predictæˆ–è€…trackï¼Œå¦‚æœæ˜¯ï¼Œåˆ™is_cliä¸ºTrue

        custom = {"conf": 0.25, "batch": 1, "save": is_cli, "mode": "predict"}  # é…ç½®é»˜è®¤çš„é¢„æµ‹å‚æ•°
        #ç½®ä¿¡åº¦é˜ˆå€¼0.25ï¼Œbatchå¤§å°ä¸º1ï¼Œä¿å­˜ç»“æœç”±is_cliå†³å®šï¼Œæ¨¡å¼ä¸ºpredict
        args = {**self.overrides, **custom, **kwargs}  # highest priority args on the right
        #åˆå¹¶ä¸‰ç»„å‚æ•°ï¼Œä¼˜å…ˆçº§ä»ä½åˆ°é«˜ä¸ºï¼š self.overrides < custom < kwargs
        prompts = args.pop("prompts", None)  # for SAM-type models

        if not self.predictor:
            #è¿è¡Œmy_detect.pyæ—¶ï¼Œä¼šè¿›å…¥è¿™ä¸ªifè¯­å¥ï¼Œæ­¤æ—¶self.predictorä¸ºNone
            self.predictor = (predictor or self._smart_load("predictor"))(overrides=args, _callbacks=self.callbacks)
            #å¦‚æœ predictor ä¸º None ï¼Œåˆ™è°ƒç”¨ self._smart_load("predictor")
            #ä½¿ç”¨é…ç½®å‚æ•° args å’Œå›è°ƒå‡½æ•° _callbacks åˆå§‹åŒ–é¢„æµ‹å™¨ predictor
            #è®¾ç½®é¢„æµ‹å™¨æ¨¡å‹å¹¶é…ç½®æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆç”± is_cli å†³å®šï¼‰
            print('\nModel.predictorç±»å±æ€§è¢«åˆå§‹åŒ–','è®¾ç½®é¢„æµ‹å™¨æ¨¡å‹ä¸ºï¼š',type(self.predictor))
            self.predictor.setup_model(model=self.model, verbose=is_cli)#self.model=DetectionModelç±»

        else:  # only update args if predictor is already setup
            self.predictor.args = get_cfg(self.predictor.args, args)
            if "project" in args or "name" in args:
                self.predictor.save_dir = get_save_dir(self.predictor.args)

        #è¿è¡Œmy_detect.pyæ—¶ï¼Œä¸ä¼šè¿›å…¥è¿™ä¸ªifè¯­å¥    
        if prompts and hasattr(self.predictor, "set_prompts"):  # for SAM-type models
            self.predictor.set_prompts(prompts)


        #self.predictor é»˜è®¤ä¸º<ultralytics.models.yolo.detect.predict.DetectionPredictor object at 0x000001F7268FCE50>
        
        if  is_cli:
            return self.predictor.predict_cli(source=source)
        else:  #æ¨ç†æ—¶é»˜è®¤is_cli=False
            result=self.predictor(source=source, stream=stream)#è¿”å›ä¸€ä¸ªç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœæ˜¯ä¸€ä¸ªResultså¯¹è±¡
            #ultralytics.engine.results.Results object list
            #è‡ªå®šä¹‰img_data
            img_data=result[0].boxes.data    #æ•°æ®ä¸ºtorch.Tensorï¼Œshapeä¸º[1, 6]ï¼Œè¡¨ç¤ºæ£€æµ‹æ¡†çš„åæ ‡ã€ç½®ä¿¡åº¦ã€ç±»åˆ«
            return result

       
############################################################################################################################################################################################


    @staticmethod
    def _reset_ckpt_args(args: dict) -> dict:
        """
        åœ¨åŠ è½½ PyTorch æ¨¡å‹æ£€æŸ¥ç‚¹æ—¶é‡ç½®ç‰¹å®šå‚æ•°ã€‚

        è¯¥é™æ€æ–¹æ³•è¿‡æ»¤è¾“å…¥å‚æ•°å­—å…¸ï¼Œä»…ä¿ç•™ä¸€ç»„è¢«è®¤ä¸ºå¯¹æ¨¡å‹åŠ è½½é‡è¦çš„é”®ã€‚
        å®ƒç”¨äºç¡®ä¿åœ¨ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æ—¶åªä¿ç•™ç›¸å…³å‚æ•°ï¼Œä¸¢å¼ƒä»»ä½•ä¸å¿…è¦æˆ–å¯èƒ½å†²çªçš„è®¾ç½®ã€‚

        å‚æ•°:
            args (dict): åŒ…å«å„ç§æ¨¡å‹å‚æ•°å’Œè®¾ç½®çš„å­—å…¸ã€‚

        è¿”å›:
            (dict): ä¸€ä¸ªæ–°çš„å­—å…¸ï¼Œä»…åŒ…å«è¾“å…¥å‚æ•°ä¸­æŒ‡å®šçš„é”®ã€‚

        ç¤ºä¾‹:
            >>> original_args = {"imgsz": 640, "data": "coco.yaml", "task": "detect", "batch": 16, "epochs": 100}
            >>> reset_args = Model._reset_ckpt_args(original_args)
            >>> print(reset_args)
            {'imgsz': 640, 'data': 'coco.yaml', 'task': 'detect'}
        """
        include = {"imgsz", "data", "task", "single_cls"}  # only remember these arguments when loading a PyTorch model
        return {k: v for k, v in args.items() if k in include}

    # def __getattr__(self, attr):
    #    """Raises error if object has no requested attribute."""
    #    name = self.__class__.__name__
    #    raise AttributeError(f"'{name}' object has no attribute '{attr}'. See valid attributes below.\n{self.__doc__}")

############################################################################################################################################################################################




    def _smart_load(self, key: str):
        print("\nModel._smart_load()è¢«è°ƒç”¨ æ ¹æ®æ¨¡å‹ä»»åŠ¡åŠ è½½ç›¸åº”çš„æ¨¡å—")
        """
        æ ¹æ®æ¨¡å‹ä»»åŠ¡åŠ è½½ç›¸åº”çš„æ¨¡å—ã€‚

        è¯¥æ–¹æ³•æ ¹æ®æ¨¡å‹å½“å‰ä»»åŠ¡å’Œæä¾›çš„ key åŠ¨æ€é€‰æ‹©å¹¶è¿”å›æ­£ç¡®çš„æ¨¡å—ï¼ˆmodelã€trainerã€validator æˆ– predictorï¼‰ã€‚
        å®ƒä½¿ç”¨ task_map å±æ€§æ¥ç¡®å®šè¦åŠ è½½çš„æ­£ç¡®æ¨¡å—ã€‚

        å‚æ•°:
            key (str): è¦åŠ è½½çš„æ¨¡å—ç±»å‹ã€‚å¿…é¡»æ˜¯ 'model'ã€'trainer'ã€'validator' æˆ– 'predictor' ä¹‹ä¸€ã€‚

        è¿”å›:
            (object): ä¸æŒ‡å®š key å’Œå½“å‰ä»»åŠ¡å¯¹åº”çš„å·²åŠ è½½æ¨¡å—ã€‚

        å¼‚å¸¸:
            NotImplementedError: å¦‚æœå½“å‰ä»»åŠ¡ä¸æ”¯æŒæŒ‡å®šçš„ keyã€‚

        ç¤ºä¾‹:
            >>> model = Model(task="detect")
            >>> predictor = model._smart_load("predictor")
            >>> trainer = model._smart_load("trainer")

        æ³¨æ„:
            - è¯¥æ–¹æ³•é€šå¸¸ç”± Model ç±»çš„å…¶ä»–æ–¹æ³•å†…éƒ¨ä½¿ç”¨ã€‚
            - task_map å±æ€§åº”æ­£ç¡®åˆå§‹åŒ–ï¼ŒåŒ…å«æ¯ä¸ªä»»åŠ¡çš„æ­£ç¡®æ˜ å°„ã€‚
        """
        try:
            return self.task_map[self.task][key]
        except Exception as e:
            name = self.__class__.__name__
            mode = inspect.stack()[1][3]  # get the function name.
            raise NotImplementedError(
                emojis(f"WARNING âš ï¸ '{name}' model does not support '{mode}' mode for '{self.task}' task yet.")
            ) from e
############################################################################################################################################################################################

    def __getattr__(self, name):
        """
        Enables accessing model attributes directly through the Model class.

        This method provides a way to access attributes of the underlying model directly through the Model class
        instance. It first checks if the requested attribute is 'model', in which case it returns the model from
        the module dictionary. Otherwise, it delegates the attribute lookup to the underlying model.

        Args:
            name (str): The name of the attribute to retrieve.

        Returns:
            (Any): The requested attribute value.

        Raises:
            AttributeError: If the requested attribute does not exist in the model.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> print(model.stride)
            >>> print(model.task)
        """
        return self._modules["model"] if name == "model" else getattr(self.model, name)

############################################################################################################################################################################################
############################################################################################################################################################################################
############################################################################################################################################################################################
############################################################################################################################################################################################
############################################################################################################################################################################################


    @property
    def task_map(self) -> dict:
        """
        æä¾›ä»æ¨¡å‹ä»»åŠ¡åˆ°ä¸åŒæ¨¡å¼ä¸‹å¯¹åº”ç±»çš„æ˜ å°„ã€‚

        è¯¥å±æ€§æ–¹æ³•è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå°†æ¯ä¸ªæ”¯æŒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚æ£€æµ‹ã€åˆ†å‰²ã€åˆ†ç±»ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªåµŒå¥—å­—å…¸ã€‚
        åµŒå¥—å­—å…¸åŒ…å«ä¸åŒæ“ä½œæ¨¡å¼ï¼ˆmodelã€trainerã€validatorã€predictorï¼‰åˆ°å®ƒä»¬å„è‡ªç±»å®ç°çš„æ˜ å°„ã€‚

        è¯¥æ˜ å°„å…è®¸æ ¹æ®æ¨¡å‹çš„ä»»åŠ¡å’Œæ‰€éœ€çš„æ“ä½œæ¨¡å¼åŠ¨æ€åŠ è½½é€‚å½“çš„ç±»ã€‚è¿™ä¸ºå¤„ç† Ultralytics æ¡†æ¶ä¸­çš„å„ç§ä»»åŠ¡å’Œæ¨¡å¼æä¾›äº†çµæ´»ä¸”å¯æ‰©å±•çš„æ¶æ„ã€‚

        è¿”å›:
            (Dict[str, Dict[str, Any]]): ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯ä»»åŠ¡åç§°ï¼ˆstrï¼‰ï¼Œå€¼æ˜¯åµŒå¥—å­—å…¸ã€‚
            æ¯ä¸ªåµŒå¥—å­—å…¸åŒ…å« 'model'ã€'trainer'ã€'validator' å’Œ 'predictor' é”®ï¼Œæ˜ å°„åˆ°å®ƒä»¬å„è‡ªçš„ç±»å®ç°ã€‚

        ç¤ºä¾‹:
            >>> model = Model()
            >>> task_map = model.task_map
            >>> detect_class_map = task_map["detect"]
            >>> segment_class_map = task_map["segment"]

        æ³¨æ„:
            è¯¥æ–¹æ³•çš„å®é™…å®ç°å¯èƒ½å›  Ultralytics æ¡†æ¶æ”¯æŒçš„å…·ä½“ä»»åŠ¡å’Œç±»è€Œå¼‚ã€‚
            æ–‡æ¡£å­—ç¬¦ä¸²æä¾›äº†é¢„æœŸè¡Œä¸ºå’Œç»“æ„çš„ä¸€èˆ¬æè¿°ã€‚
        """
        raise NotImplementedError("Please provide task map for your model!")

    def eval(self):
        """
        Sets the model to evaluation mode.

        This method changes the model's mode to evaluation, which affects layers like dropout and batch normalization
        that behave differently during training and evaluation.

        Returns:
            (Model): The model instance with evaluation mode set.

        Examples:
            >> model = YOLO("yolo11n.pt")
            >> model.eval()
        """
        self.model.eval()
        return self





    def __call__(
        self,
        source: Union[str, Path, int, Image.Image, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> list:
        print("ä½¿ç”¨äº†Modelçš„callæ–¹æ³•")#æ£€æµ‹ä»»åŠ¡ä¸­å¹¶æœªä½¿ç”¨
        
        """
        predict æ–¹æ³•çš„åˆ«åï¼Œä½¿æ¨¡å‹å®ä¾‹å¯ä»¥ç›´æ¥è°ƒç”¨è¿›è¡Œé¢„æµ‹ã€‚

        è¯¥æ–¹æ³•é€šè¿‡å…è®¸ç›´æ¥è°ƒç”¨æ¨¡å‹å®ä¾‹æ¥ç®€åŒ–é¢„æµ‹è¿‡ç¨‹ï¼Œåªéœ€ä¼ å…¥å¿…è¦çš„å‚æ•°å³å¯ã€‚

        å‚æ•°:
            source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): 
                è¦è¿›è¡Œé¢„æµ‹çš„å›¾åƒæ¥æºã€‚å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€URLã€PIL å›¾åƒã€numpy æ•°ç»„ã€PyTorch å¼ é‡æˆ–è¿™äº›ç±»å‹çš„åˆ—è¡¨/å…ƒç»„ã€‚
            stream (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†è¾“å…¥æºè§†ä¸ºè¿ç»­æµè¿›è¡Œé¢„æµ‹ã€‚
            **kwargs: ç”¨äºé…ç½®é¢„æµ‹è¿‡ç¨‹çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            (List[ultralytics.engine.results.Results]): é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœå°è£…åœ¨ä¸€ä¸ª Results å¯¹è±¡ä¸­ã€‚

        ç¤ºä¾‹:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model("https://ultralytics.com/images/bus.jpg")
            >>> for r in results:
            ...     print(f"Detected {len(r)} objects in image")
        """

        return self.predict(source, stream, **kwargs)

    def _new(self, cfg: str, task=None, model=None, verbose=False) -> None:#æ¨ç†æ—¶æœªè¢«è°ƒç”¨

        print("Model._new()è¢«è°ƒç”¨")

        """
        åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹å¹¶ä»æ¨¡å‹å®šä¹‰ä¸­æ¨æ–­ä»»åŠ¡ç±»å‹ã€‚

        è¯¥æ–¹æ³•æ ¹æ®æä¾›çš„é…ç½®æ–‡ä»¶åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹ã€‚å®ƒåŠ è½½æ¨¡å‹é…ç½®ï¼Œå¦‚æœæœªæŒ‡å®šä»»åŠ¡ç±»å‹åˆ™ä»é…ç½®ä¸­æ¨æ–­ï¼Œå¹¶ä½¿ç”¨ä»»åŠ¡æ˜ å°„ä¸­çš„é€‚å½“ç±»åˆå§‹åŒ–æ¨¡å‹ã€‚

        å‚æ•°:
            cfg (str): YAML æ ¼å¼çš„æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
            task (str | None): æ¨¡å‹çš„å…·ä½“ä»»åŠ¡ç±»å‹ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä»é…ç½®ä¸­æ¨æ–­ã€‚
            model (torch.nn.Module | None): è‡ªå®šä¹‰æ¨¡å‹å®ä¾‹ã€‚å¦‚æœæä¾›ï¼Œåˆ™ä½¿ç”¨è¯¥å®ä¾‹è€Œä¸æ˜¯åˆ›å»ºæ–°æ¨¡å‹ã€‚
            verbose (bool): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨åŠ è½½æœŸé—´æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ã€‚

        å¼‚å¸¸:
            ValueError: å¦‚æœé…ç½®æ–‡ä»¶æ— æ•ˆæˆ–æ— æ³•æ¨æ–­ä»»åŠ¡ç±»å‹ã€‚
            ImportError: å¦‚æœæŒ‡å®šä»»åŠ¡æ‰€éœ€çš„ä¾èµ–é¡¹æœªå®‰è£…ã€‚

        ç¤ºä¾‹:
            >>> model = Model()
            >>> model._new("yolo11n.yaml", task="detect", verbose=True)
        """
        cfg_dict = yaml_model_load(cfg)
        self.cfg = cfg
        self.task = task or guess_model_task(cfg_dict)
        self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
        self.overrides["model"] = self.cfg
        self.overrides["task"] = self.task

        # Below added to allow export from YAMLs
        self.model.args = {**DEFAULT_CFG_DICT, **self.overrides}  # combine default and model args (prefer model args)
        self.model.task = self.task
        self.model_name = cfg


    def val(
        self,
        validator=None,
        **kwargs: Any,
    ):
        """
        ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†å’ŒéªŒè¯é…ç½®å¯¹æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚

        è¯¥æ–¹æ³•ç®€åŒ–äº†æ¨¡å‹éªŒè¯è¿‡ç¨‹ï¼Œå…è®¸é€šè¿‡å„ç§è®¾ç½®è¿›è¡Œè‡ªå®šä¹‰ã€‚å®ƒæ”¯æŒä½¿ç”¨è‡ªå®šä¹‰éªŒè¯å™¨æˆ–é»˜è®¤éªŒè¯æ–¹æ³•è¿›è¡ŒéªŒè¯ã€‚
        è¯¥æ–¹æ³•ç»“åˆäº†é»˜è®¤é…ç½®ã€æ–¹æ³•ç‰¹å®šçš„é»˜è®¤å€¼å’Œç”¨æˆ·æä¾›çš„å‚æ•°æ¥é…ç½®éªŒè¯è¿‡ç¨‹ã€‚

        å‚æ•°:
            validator (ultralytics.engine.validator.BaseValidator | None): ç”¨äºéªŒè¯æ¨¡å‹çš„è‡ªå®šä¹‰éªŒè¯å™¨ç±»çš„å®ä¾‹ã€‚
            **kwargs: ç”¨äºè‡ªå®šä¹‰éªŒè¯è¿‡ç¨‹çš„ä»»æ„å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            (ultralytics.utils.metrics.DetMetrics): ä»éªŒè¯è¿‡ç¨‹ä¸­è·å¾—çš„éªŒè¯æŒ‡æ ‡ã€‚

        å¼‚å¸¸:
            AssertionError: å¦‚æœæ¨¡å‹ä¸æ˜¯ PyTorch æ¨¡å‹ã€‚

        ç¤ºä¾‹:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.val(data="coco8.yaml", imgsz=640)
            >>> print(results.box.map)  # æ‰“å° mAP50-95
        """
        custom = {"rect": True}  # method defaults
        args = {**self.overrides, **custom, **kwargs, "mode": "val"}  # highest priority args on the right

        validator = (validator or self._smart_load("validator"))(args=args, _callbacks=self.callbacks)
        validator(model=self.model)
        self.metrics = validator.metrics#
        return validator.metrics

    def benchmark(
        self,
        **kwargs: Any,
    ):
        """
        Benchmarks the model across various export formats to evaluate performance.

        This method assesses the model's performance in different export formats, such as ONNX, TorchScript, etc.
        It uses the 'benchmark' function from the ultralytics.utils.benchmarks module. The benchmarking is
        configured using a combination of default configuration values, model-specific arguments, method-specific
        defaults, and any additional user-provided keyword arguments.

        Args:
            **kwargs: Arbitrary keyword arguments to customize the benchmarking process. These are combined with
                default configurations, model-specific arguments, and method defaults. Common options include:
                - data (str): Path to the dataset for benchmarking.
                - imgsz (int | List[int]): Image size for benchmarking.
                - half (bool): Whether to use half-precision (FP16) mode.
                - int8 (bool): Whether to use int8 precision mode.
                - device (str): Device to run the benchmark on (e.g., 'cpu', 'cuda').
                - verbose (bool): Whether to print detailed benchmark information.
                - format (str): Export format name for specific benchmarking

        Returns:
            (Dict): A dictionary containing the results of the benchmarking process, including metrics for
                different export formats.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.benchmark(data="coco8.yaml", imgsz=640, half=True)
            >>> print(results)
        """
        self._check_is_pytorch_model()
        from ultralytics.utils.benchmarks import benchmark

        custom = {"verbose": False}  # method defaults
        args = {**DEFAULT_CFG_DICT, **self.model.args, **custom, **kwargs, "mode": "benchmark"}
        return benchmark(
            model=self,
            data=kwargs.get("data"),  # if no 'data' argument passed set data=None for default datasets
            imgsz=args["imgsz"],
            half=args["half"],
            int8=args["int8"],
            device=args["device"],
            verbose=kwargs.get("verbose", False),
            format=kwargs.get("format", ""),
        )



    def _check_is_pytorch_model(self) -> None:#æ¨ç†æ—¶æœªè¢«è°ƒç”¨

        print("Model._check_is_pytorch_model()è¢«è°ƒç”¨",'æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºPyTorchæ¨¡å‹')
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸º PyTorch æ¨¡å‹ï¼Œå¦‚æœä¸æ˜¯åˆ™æŠ›å‡º TypeError å¼‚å¸¸ã€‚

        è¯¥æ–¹æ³•éªŒè¯æ¨¡å‹æ˜¯å¦ä¸º PyTorch æ¨¡å—æˆ– .pt æ–‡ä»¶ã€‚ç”¨äºç¡®ä¿éœ€è¦ PyTorch æ¨¡å‹çš„ç‰¹å®šæ“ä½œä»…åœ¨å…¼å®¹çš„æ¨¡å‹ç±»å‹ä¸Šæ‰§è¡Œã€‚

        å¼‚å¸¸:
            TypeError: å¦‚æœæ¨¡å‹ä¸æ˜¯ PyTorch æ¨¡å—æˆ– .pt æ–‡ä»¶ã€‚é”™è¯¯ä¿¡æ¯ä¼šè¯¦ç»†è¯´æ˜æ”¯æŒçš„æ¨¡å‹æ ¼å¼å’Œæ“ä½œã€‚

        ç¤ºä¾‹:
            >>> model = Model("yolo11n.pt")
            >>> model._check_is_pytorch_model()  # ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            >>> model = Model("yolo11n.onnx")
            >>> model._check_is_pytorch_model()  # æŠ›å‡º TypeError å¼‚å¸¸
        """
        pt_str = isinstance(self.model, (str, Path)) and Path(self.model).suffix == ".pt"
        pt_module = isinstance(self.model, torch.nn.Module)
        if not (pt_module or pt_str):
            raise TypeError(
                f"model='{self.model}' should be a *.pt PyTorch model to run this method, but is a different format. "
                f"PyTorch models can train, val, predict and export, i.e. 'model.train(data=...)', but exported "
                f"formats like ONNX, TensorRT etc. only support 'predict' and 'val' modes, "
                f"i.e. 'yolo predict model=yolo11n.onnx'.\nTo run CUDA or MPS inference please pass the device "
                f"argument directly in your inference command, i.e. 'model.predict(source=..., device=0)'"
            )
    def export(
        self,
        **kwargs: Any,
    ) -> str:
        """
        Exports the model to a different format suitable for deployment.

        This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment
        purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method
        defaults, and any additional arguments provided.

        Args:
            **kwargs: Arbitrary keyword arguments to customize the export process. These are combined with
                the model's overrides and method defaults. Common arguments include:
                format (str): Export format (e.g., 'onnx', 'engine', 'coreml').
                half (bool): Export model in half-precision.
                int8 (bool): Export model in int8 precision.
                device (str): Device to run the export on.
                workspace (int): Maximum memory workspace size for TensorRT engines.
                nms (bool): Add Non-Maximum Suppression (NMS) module to model.
                simplify (bool): Simplify ONNX model.

        Returns:
            (str): The path to the exported model file.

        Raises:
            AssertionError: If the model is not a PyTorch model.
            ValueError: If an unsupported export format is specified.
            RuntimeError: If the export process fails due to errors.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> model.export(format="onnx", dynamic=True, simplify=True)
            'path/to/exported/model.onnx'
        """
        self._check_is_pytorch_model()
        from .exporter import Exporter

        custom = {
            "imgsz": self.model.args["imgsz"],
            "batch": 1,
            "data": None,
            "device": None,  # reset to avoid multi-GPU errors
            "verbose": False,
        }  # method defaults
        args = {**self.overrides, **custom, **kwargs, "mode": "export"}  # highest priority args on the right
        return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)

    def reset_weights(self) -> "Model":#æ¨ç†æ—¶æœªè¢«è°ƒç”¨

        """
        Resets the model's weights to their initial state.

        This method iterates through all modules in the model and resets their parameters if they have a
        'reset_parameters' method. It also ensures that all parameters have 'requires_grad' set to True,
        enabling them to be updated during training.

        Returns:
            (Model): The instance of the class with reset weights.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = Model("yolo11n.pt")
            >>> model.reset_weights()
        """
        self._check_is_pytorch_model()
        for m in self.model.modules():
            if hasattr(m, "reset_parameters"):
                m.reset_parameters()
        for p in self.model.parameters():
            p.requires_grad = True
        return self

    def load(self, weights: Union[str, Path] = "yolo11n.pt") -> "Model":#æ¨ç†æ—¶æœªè¢«è°ƒç”¨
        """
        Loads parameters from the specified weights file into the model.

        This method supports loading weights from a file or directly from a weights object. It matches parameters by
        name and shape and transfers them to the model.

        Args:
            weights (Union[str, Path]): Path to the weights file or a weights object.

        Returns:
            (Model): The instance of the class with loaded weights.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = Model()
            >>> model.load("yolo11n.pt")
            >>> model.load(Path("path/to/weights.pt"))
        """
        self._check_is_pytorch_model()
        if isinstance(weights, (str, Path)):
            self.overrides["pretrained"] = weights  # remember the weights for DDP training
            weights, self.ckpt = attempt_load_one_weight(weights)
        self.model.load(weights)
        return self

    def save(self, filename: Union[str, Path] = "saved_model.pt") -> None:
        """
        Saves the current model state to a file.

        This method exports the model's checkpoint (ckpt) to the specified filename. It includes metadata such as
        the date, Ultralytics version, license information, and a link to the documentation.

        Args:
            filename (Union[str, Path]): The name of the file to save the model to.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = Model("yolo11n.pt")
            >>> model.save("my_model.pt")
        """
        self._check_is_pytorch_model()
        from copy import deepcopy
        from datetime import datetime

        from ultralytics import __version__

        updates = {
            "model": deepcopy(self.model).half() if isinstance(self.model, torch.nn.Module) else self.model,
            "date": datetime.now().isoformat(),
            "version": __version__,
            "license": "AGPL-3.0 License (https://ultralytics.com/license)",
            "docs": "https://docs.ultralytics.com",
        }
        torch.save({**self.ckpt, **updates}, filename)

    def info(self, detailed: bool = False, verbose: bool = True):
        """
        Logs or returns model information.

        This method provides an overview or detailed information about the model, depending on the arguments
        passed. It can control the verbosity of the output and return the information as a list.

        Args:
            detailed (bool): If True, shows detailed information about the model layers and parameters.
            verbose (bool): If True, prints the information. If False, returns the information as a list.

        Returns:
            (List[str]): A list of strings containing various types of information about the model, including
                model summary, layer details, and parameter counts. Empty if verbose is True.

        Raises:
            TypeError: If the model is not a PyTorch model.

        Examples:
            >>> model = Model("yolo11n.pt")
            >>> model.info()  # Prints model summary
            >>> info_list = model.info(detailed=True, verbose=False)  # Returns detailed info as a list
        """
        self._check_is_pytorch_model()
        return self.model.info(detailed=detailed, verbose=verbose)

    def fuse(self):
        """
        Fuses Conv2d and BatchNorm2d layers in the model for optimized inference.

        This method iterates through the model's modules and fuses consecutive Conv2d and BatchNorm2d layers
        into a single layer. This fusion can significantly improve inference speed by reducing the number of
        operations and memory accesses required during forward passes.

        The fusion process typically involves folding the BatchNorm2d parameters (mean, variance, weight, and
        bias) into the preceding Conv2d layer's weights and biases. This results in a single Conv2d layer that
        performs both convolution and normalization in one step.

        Raises:
            TypeError: If the model is not a PyTorch torch.nn.Module.

        Examples:
            >>> model = Model("yolo11n.pt")
            >>> model.fuse()
            >>> # Model is now fused and ready for optimized inference
        """
        self._check_is_pytorch_model()
        self.model.fuse()

    def embed(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> list:#æ¨ç†æ—¶æœªè¢«è°ƒç”¨

        print("Model.embed()è¢«è°ƒç”¨",'å›¾åƒæºsourceç”ŸæˆåµŒå…¥')

        """
        åŸºäºæä¾›çš„å›¾åƒæºç”Ÿæˆå›¾åƒåµŒå…¥ã€‚

        è¯¥æ–¹æ³•æ˜¯ 'predict()' æ–¹æ³•çš„å°è£…ï¼Œä¸“æ³¨äºä»å›¾åƒæºç”ŸæˆåµŒå…¥ã€‚å®ƒå…è®¸é€šè¿‡å„ç§å…³é”®å­—å‚æ•°è‡ªå®šä¹‰åµŒå…¥è¿‡ç¨‹ã€‚

        å‚æ•°:
            source (str | Path | int | List | Tuple | np.ndarray | torch.Tensor): 
                ç”¨äºç”ŸæˆåµŒå…¥çš„å›¾åƒæºã€‚å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€URLã€PIL å›¾åƒã€numpy æ•°ç»„ç­‰ã€‚
            stream (bool): å¦‚æœä¸º Trueï¼Œåˆ™è¿›è¡Œæµå¼é¢„æµ‹ã€‚
            **kwargs: ç”¨äºé…ç½®åµŒå…¥è¿‡ç¨‹çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            (List[torch.Tensor]): åŒ…å«å›¾åƒåµŒå…¥çš„åˆ—è¡¨ã€‚

        å¼‚å¸¸:
            AssertionError: å¦‚æœæ¨¡å‹ä¸æ˜¯ PyTorch æ¨¡å‹ã€‚

        ç¤ºä¾‹:
            >>> model = YOLO("yolo11n.pt")
            >>> image = "https://ultralytics.com/images/bus.jpg"
            >>> embeddings = model.embed(image)
            >>> print(embeddings[0].shape)
        """
        if not kwargs.get("embed"):
            kwargs["embed"] = [len(self.model.model) - 2]  # embed second-to-last layer if no indices passed
        return self.predict(source, stream, **kwargs)



    def track(
        self,
        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,
        stream: bool = False,
        persist: bool = False,
        **kwargs: Any,
    ) -> List[Results]:
        """
        ä½¿ç”¨æ³¨å†Œçš„è·Ÿè¸ªå™¨å¯¹æŒ‡å®šçš„è¾“å…¥æºè¿›è¡Œç›®æ ‡è·Ÿè¸ªã€‚

        è¯¥æ–¹æ³•ä½¿ç”¨æ¨¡å‹çš„é¢„æµ‹å™¨å’Œå¯é€‰çš„æ³¨å†Œè·Ÿè¸ªå™¨æ‰§è¡Œç›®æ ‡è·Ÿè¸ªã€‚å®ƒå¤„ç†å„ç§è¾“å…¥æºï¼Œå¦‚æ–‡ä»¶è·¯å¾„æˆ–è§†é¢‘æµï¼Œå¹¶æ”¯æŒé€šè¿‡å…³é”®å­—å‚æ•°è¿›è¡Œè‡ªå®šä¹‰ã€‚å¦‚æœå°šæœªå­˜åœ¨è·Ÿè¸ªå™¨ï¼Œè¯¥æ–¹æ³•ä¼šæ³¨å†Œè·Ÿè¸ªå™¨ï¼Œå¹¶å¯ä»¥åœ¨ä¸åŒè°ƒç”¨ä¹‹é—´æŒä¹…åŒ–è·Ÿè¸ªå™¨ã€‚

        å‚æ•°:
            source (Union[str, Path, int, List, Tuple, np.ndarray, torch.Tensor], optional): 
                ç”¨äºç›®æ ‡è·Ÿè¸ªçš„è¾“å…¥æºã€‚å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€URL æˆ–è§†é¢‘æµã€‚
            stream (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†è¾“å…¥æºè§†ä¸ºè¿ç»­è§†é¢‘æµã€‚é»˜è®¤ä¸º Falseã€‚
            persist (bool): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨ä¸åŒè°ƒç”¨ä¹‹é—´æŒä¹…åŒ–è·Ÿè¸ªå™¨ã€‚é»˜è®¤ä¸º Falseã€‚
            **kwargs: ç”¨äºé…ç½®è·Ÿè¸ªè¿‡ç¨‹çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

        è¿”å›:
            (List[ultralytics.engine.results.Results]): è·Ÿè¸ªç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœéƒ½æ˜¯ä¸€ä¸ª Results å¯¹è±¡ã€‚

        å¼‚å¸¸:
            AttributeError: å¦‚æœé¢„æµ‹å™¨æ²¡æœ‰æ³¨å†Œçš„è·Ÿè¸ªå™¨ã€‚

        ç¤ºä¾‹:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.track(source="path/to/video.mp4", show=True)
            >>> for r in results:
            ...     print(r.boxes.id)  # æ‰“å°è·Ÿè¸ª ID
        """
        if not hasattr(self.predictor, "trackers"):
            from ultralytics.trackers import register_tracker

            register_tracker(self, persist)
        kwargs["conf"] = kwargs.get("conf") or 0.1  # ByteTrack-based method needs low confidence predictions as input
        kwargs["batch"] = kwargs.get("batch") or 1  # batch-size 1 for tracking in videos
        kwargs["mode"] = "track"
        return self.predict(source=source, stream=stream, **kwargs)




    @property
    def transforms(self):
        """
        Retrieves the transformations applied to the input data of the loaded model.

        This property returns the transformations if they are defined in the model. The transforms
        typically include preprocessing steps like resizing, normalization, and data augmentation
        that are applied to input data before it is fed into the model.

        Returns:
            (object | None): The transform object of the model if available, otherwise None.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> transforms = model.transforms
            >>> if transforms:
            ...     print(f"Model transforms: {transforms}")
            ... else:
            ...     print("No transforms defined for this model.")
        """
        return self.model.transforms if hasattr(self.model, "transforms") else None

    def add_callback(self, event: str, func) -> None:
        """
        Adds a callback function for a specified event.

        This method allows registering custom callback functions that are triggered on specific events during
        model operations such as training or inference. Callbacks provide a way to extend and customize the
        behavior of the model at various stages of its lifecycle.

        Args:
            event (str): The name of the event to attach the callback to. Must be a valid event name recognized
                by the Ultralytics framework.
            func (Callable): The callback function to be registered. This function will be called when the
                specified event occurs.

        Raises:
            ValueError: If the event name is not recognized or is invalid.

        Examples:
            >>> def on_train_start(trainer):
            ...     print("Training is starting!")
            >>> model = YOLO("yolo11n.pt")
            >>> model.add_callback("on_train_start", on_train_start)
            >>> model.train(data="coco8.yaml", epochs=1)
        """
        self.callbacks[event].append(func)

    def clear_callback(self, event: str) -> None:
        """
        Clears all callback functions registered for a specified event.

        This method removes all custom and default callback functions associated with the given event.
        It resets the callback list for the specified event to an empty list, effectively removing all
        registered callbacks for that event.

        Args:
            event (str): The name of the event for which to clear the callbacks. This should be a valid event name
                recognized by the Ultralytics callback system.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> model.add_callback("on_train_start", lambda: print("Training started"))
            >>> model.clear_callback("on_train_start")
            >>> # All callbacks for 'on_train_start' are now removed

        Notes:
            - This method affects both custom callbacks added by the user and default callbacks
              provided by the Ultralytics framework.
            - After calling this method, no callbacks will be executed for the specified event
              until new ones are added.
            - Use with caution as it removes all callbacks, including essential ones that might
              be required for proper functioning of certain operations.
        """
        self.callbacks[event] = []

    def reset_callbacks(self) -> None:
        """
        Resets all callbacks to their default functions.

        This method reinstates the default callback functions for all events, removing any custom callbacks that were
        previously added. It iterates through all default callback events and replaces the current callbacks with the
        default ones.

        The default callbacks are defined in the 'callbacks.default_callbacks' dictionary, which contains predefined
        functions for various events in the model's lifecycle, such as on_train_start, on_epoch_end, etc.

        This method is useful when you want to revert to the original set of callbacks after making custom
        modifications, ensuring consistent behavior across different runs or experiments.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> model.add_callback("on_train_start", custom_function)
            >>> model.reset_callbacks()
            # All callbacks are now reset to their default functions
        """
        for event in callbacks.default_callbacks.keys():
            self.callbacks[event] = [callbacks.default_callbacks[event][0]]








    def train(
        self,
        trainer=None,
        **kwargs: Any,
    ):
        """
        Trains the model using the specified dataset and training configuration.

        This method facilitates model training with a range of customizable settings. It supports training with a
        custom trainer or the default training approach. The method handles scenarios such as resuming training
        from a checkpoint, integrating with Ultralytics HUB, and updating model and configuration after training.

        When using Ultralytics HUB, if the session has a loaded model, the method prioritizes HUB training
        arguments and warns if local arguments are provided. It checks for pip updates and combines default
        configurations, method-specific defaults, and user-provided arguments to configure the training process.

        Args:
            trainer (BaseTrainer | None): Custom trainer instance for model training. If None, uses default.
            **kwargs: Arbitrary keyword arguments for training configuration. Common options include:
                data (str): Path to dataset configuration file.
                epochs (int): Number of training epochs.
                batch_size (int): Batch size for training.
                imgsz (int): Input image size.
                device (str): Device to run training on (e.g., 'cuda', 'cpu').
                workers (int): Number of worker threads for data loading.
                optimizer (str): Optimizer to use for training.
                lr0 (float): Initial learning rate.
                patience (int): Epochs to wait for no observable improvement for early stopping of training.

        Returns:
            (Dict | None): Training metrics if available and training is successful; otherwise, None.

        Raises:
            AssertionError: If the model is not a PyTorch model.
            PermissionError: If there is a permission issue with the HUB session.
            ModuleNotFoundError: If the HUB SDK is not installed.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.train(data="coco8.yaml", epochs=3)
        """
        self._check_is_pytorch_model()
        if hasattr(self.session, "model") and self.session.model.id:  # Ultralytics HUB session with loaded model
            if any(kwargs):
                LOGGER.warning("WARNING âš ï¸ using HUB training arguments, ignoring local training arguments.")
            kwargs = self.session.train_args  # overwrite kwargs

        checks.check_pip_update_available()

        overrides = yaml_load(checks.check_yaml(kwargs["cfg"])) if kwargs.get("cfg") else self.overrides
        custom = {
            # NOTE: handle the case when 'cfg' includes 'data'.
            "data": overrides.get("data") or DEFAULT_CFG_DICT["data"] or TASK2DATA[self.task],
            "model": self.overrides["model"],
            "task": self.task,
        }  # method defaults
        args = {**overrides, **custom, **kwargs, "mode": "train"}  # highest priority args on the right
        if args.get("resume"):
            args["resume"] = self.ckpt_path


        #self.trainer ä¸º<ultralytics.models.yolo.detect.train.DetectionTrainer object at 0x00000290BF536CD0>
        self.trainer = (trainer or self._smart_load("trainer"))(overrides=args, _callbacks=self.callbacks)
        if not args.get("resume"):  # å¦‚æœä¸æ˜¯ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
            # æ‰‹åŠ¨è®¾ç½®æ¨¡å‹ï¼šå¦‚æœå­˜åœ¨æ£€æŸ¥ç‚¹åˆ™ä½¿ç”¨å½“å‰æ¨¡å‹ä½œä¸ºæƒé‡ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)
            self.model = self.trainer.model  # æ›´æ–°æ¨¡å‹å¼•ç”¨

        self.trainer.hub_session = self.session  # é™„åŠ å¯é€‰çš„ HUB ä¼šè¯ self.sessioné»˜è®¤ä¸ºNone
        self.trainer.train()  # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
        
        # è®­ç»ƒå®Œæˆåæ›´æ–°æ¨¡å‹å’Œé…ç½®
        if RANK in {-1, 0}:  # ä»…åœ¨ä¸»è¿›ç¨‹ï¼ˆrankä¸º-1æˆ–0ï¼‰æ‰§è¡Œ
            # è·å–æœ€ä½³æˆ–æœ€åçš„æ£€æŸ¥ç‚¹
            ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last
            # åŠ è½½æ£€æŸ¥ç‚¹æƒé‡
            self.model, self.ckpt = attempt_load_one_weight(ckpt)
            # æ›´æ–°æ¨¡å‹å‚æ•°
            self.overrides = self.model.args
            # è·å–éªŒè¯å™¨æŒ‡æ ‡ï¼ˆDDPæ¨¡å¼ä¸‹å¯èƒ½æ²¡æœ‰è¿”å›æŒ‡æ ‡ï¼‰
            self.metrics = getattr(self.trainer.validator, "metrics", None)  # TODO: DDP æ¨¡å¼ä¸‹æ²¡æœ‰è¿”å›æŒ‡æ ‡
        return self.metrics  # è¿”å›è®­ç»ƒæŒ‡æ ‡

    def tune(
        self,
        use_ray=False,
        iterations=10,
        *args: Any,
        **kwargs: Any,
    ):
        """
        Conducts hyperparameter tuning for the model, with an option to use Ray Tune.

        This method supports two modes of hyperparameter tuning: using Ray Tune or a custom tuning method.
        When Ray Tune is enabled, it leverages the 'run_ray_tune' function from the ultralytics.utils.tuner module.
        Otherwise, it uses the internal 'Tuner' class for tuning. The method combines default, overridden, and
        custom arguments to configure the tuning process.

        Args:
            use_ray (bool): If True, uses Ray Tune for hyperparameter tuning. Defaults to False.
            iterations (int): The number of tuning iterations to perform. Defaults to 10.
            *args: Variable length argument list for additional arguments.
            **kwargs: Arbitrary keyword arguments. These are combined with the model's overrides and defaults.

        Returns:
            (Dict): A dictionary containing the results of the hyperparameter search.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> results = model.tune(use_ray=True, iterations=20)
            >>> print(results)
        """
        self._check_is_pytorch_model()
        if use_ray:
            from ultralytics.utils.tuner import run_ray_tune

            return run_ray_tune(self, max_samples=iterations, *args, **kwargs)
        else:
            from .tuner import Tuner

            custom = {}  # method defaults
            args = {**self.overrides, **custom, **kwargs, "mode": "train"}  # highest priority args on the right
            return Tuner(args=args, _callbacks=self.callbacks)(model=self, iterations=iterations)



    def _apply(self, fn) -> "Model":
        """
        Applies a function to model tensors that are not parameters or registered buffers.

        This method extends the functionality of the parent class's _apply method by additionally resetting the
        predictor and updating the device in the model's overrides. It's typically used for operations like
        moving the model to a different device or changing its precision.

        Args:
            fn (Callable): A function to be applied to the model's tensors. This is typically a method like
                to(), cpu(), cuda(), half(), or float().

        Returns:
            (Model): The model instance with the function applied and updated attributes.

        Raises:
            AssertionError: If the model is not a PyTorch model.

        Examples:
            >>> model = Model("yolo11n.pt")
            >>> model = model._apply(lambda t: t.cuda())  # Move model to GPU
        """
        self._check_is_pytorch_model()
        self = super()._apply(fn)  # noqa
        self.predictor = None  # reset predictor as device may have changed
        self.overrides["device"] = self.device  # was str(self.device) i.e. device(type='cuda', index=0) -> 'cuda:0'
        return self

    @property
    def names(self) -> Dict[int, str]:
        """
        Retrieves the class names associated with the loaded model.

        This property returns the class names if they are defined in the model. It checks the class names for validity
        using the 'check_class_names' function from the ultralytics.nn.autobackend module. If the predictor is not
        initialized, it sets it up before retrieving the names.

        Returns:
            (Dict[int, str]): A dict of class names associated with the model.

        Raises:
            AttributeError: If the model or predictor does not have a 'names' attribute.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> print(model.names)
            {0: 'person', 1: 'bicycle', 2: 'car', ...}
        """
        from ultralytics.nn.autobackend import check_class_names

        if hasattr(self.model, "names"):
            return check_class_names(self.model.names)
        if not self.predictor:  # export formats will not have predictor defined until predict() is called
            self.predictor = self._smart_load("predictor")(overrides=self.overrides, _callbacks=self.callbacks)
            self.predictor.setup_model(model=self.model, verbose=False)
        return self.predictor.model.names

    @property
    def device(self) -> torch.device:
        """
        Retrieves the device on which the model's parameters are allocated.

        This property determines the device (CPU or GPU) where the model's parameters are currently stored. It is
        applicable only to models that are instances of torch.nn.Module.

        Returns:
            (torch.device): The device (CPU/GPU) of the model.

        Raises:
            AttributeError: If the model is not a torch.nn.Module instance.

        Examples:
            >>> model = YOLO("yolo11n.pt")
            >>> print(model.device)
            device(type='cuda', index=0)  # if CUDA is available
            >>> model = model.to("cpu")
            >>> print(model.device)
            device(type='cpu')
        """
        return next(self.model.parameters()).device if isinstance(self.model, torch.nn.Module) else None
